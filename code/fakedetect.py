# -*- coding: utf-8 -*-
"""FakeDetect

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ejTBxsFvbrSjTRKMybjlDwCIC_af_dU1
"""

import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
plt.style.use('ggplot')
import seaborn as sns

#Criando vínculo entre o google drive e o código
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/FakeDetect/dataset.csv')
df.dropna(inplace=True)

tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(df['text'])
df['text'] = tokenizer.texts_to_sequences(df['text'])

X_lens = [len(x) for x in df['text'].values]
X_lens = np.array(X_lens)

#Faz o plot, mostrando a maior densidade de tamanho entre todos os artigos
sns.distplot(X_lens)

#Média de tamanho dos artigos
np.mean(X_lens)

#Desvio padrão do tamanho dos artigos
np.std(X_lens)

#Tamanho máximo de caracteres por artigo
MAX_LENGTH = 2986
np.unique((X_lens >= MAX_LENGTH), return_counts=True)

#Tokenização dos títulos
title_tokenizer = tf.keras.preprocessing.text.Tokenizer()
title_tokenizer.fit_on_texts(df['title'])
df['title'] = title_tokenizer.texts_to_sequences(df['title'])

#Tamanho máximo dentre os títulos
MAX_TITLE = 19

#Dropa a origem e o id de cada artigo
df.drop('source', axis=1, inplace=True)
df.drop('id', axis=1, inplace=True)

labels = df.pop('label')
labels.replace({'fake': 1, 'real': 0}, inplace=True)

#Divisão de dados entre treinos e testes
X_train, X_test, y_train, y_test = train_test_split(df, labels)

#Retirando titulos e textos dos artigos, para serem comparados com os dados obtidos do webscraping no twitter
X_train_title = X_train.pop('title')
X_test_title = X_test.pop('title')
X_train_text = X_train.pop('text')
X_test_text = X_test.pop('text')

#Se o título for muito curto, são adicionados zeros ao vetor. E caso forem muito longos, serão cortados.
X_train_title = tf.keras.preprocessing.sequence.pad_sequences(X_train_title,
                                              maxlen=MAX_TITLE,
                                              padding='post',
                                              truncating='post')
X_test_title = tf.keras.preprocessing.sequence.pad_sequences(X_test_title,
                                              maxlen=MAX_TITLE,
                                              padding='post',
                                              truncating='post')

#Mesma coisa se repete para os textos
X_train_text = tf.keras.preprocessing.sequence.pad_sequences(X_train_text,
                                              maxlen=MAX_LENGTH,
                                              padding='post',
                                              truncating='post')
X_test_text = tf.keras.preprocessing.sequence.pad_sequences(X_test_text,
                                              maxlen=MAX_LENGTH,
                                              padding='post',
                                              truncating='post')

#Criação do modelo
text_input = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='article_body_input')
text_embed = tf.keras.layers.Embedding(tokenizer.index_word + 1, 50, input_length=MAX_LENGTH, name='article_body_embedding')(text_input)
text_conv = tf.keras.layers.Conv1D(256, 10, name='article_body_conv')(text_embed)
text_pool = tf.keras.layers.GlobalMaxPool1D(name='article_body_pooling')(text_conv)
title_input = tf.keras.layers.Input(shape=(TITLE_LENGTH,), name = 'article_title_input')
title_embed = tf.keras.layers.Embedding(title_tokenizer.index_word + 1, 50, input_length=TITLE_LENGTH, name='article_title_embedding')(title_input)
title_conv = tf.keras.layers.Conv1D(256, 3, name='article_title_conv')(title_embed)
title_pool = tf.keras.layers.GlobalMaxPooling1D(name='article_title_pooling')(title_conv)
vector_input = tf.keras.layers.Input(shape=(21,), name='twitter_input')
concat = tf.keras.layers.concatenate([text_pool, title_pool, vector_input])
dense_100 = tf.keras.layers.Dense(100, activation='relu')(concat)
dense_50 = tf.keras.layers.Dense(50, activation='relu')(dense_100)
out_layer = tf.keras.layers.Dense(1, activation='sigmoid')(dense_50)
model = tf.keras.models.Model(inputs=[text_input, title_input, vector_input], outputs=out_layer)
model.summary()
model.compile(optimizer=tf.keras.optimizers.Adam(0.0005),
              loss='binary_crossentropy',
              metrics=['accuracy'])

#Criação do treinamento
callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc',
                                            patience=5,
                                            mode='max',
                                          restore_best_weights=True)
history = model.fit([X_train_text, X_train_title, X_train],
                     y_train,
                     epochs=100,
                     batch_size=128,
                     validation_data=(
                           [X_test_text, X_test_title, X_test],
                            y_test
                     ),
callbacks=[callback])

